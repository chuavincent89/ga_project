{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1378656",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Subreddit Classification with NLP\n",
    "> This project is to develop a classification model to classify subreddit posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd0ec0",
   "metadata": {},
   "source": [
    "## Problem Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57192cc8",
   "metadata": {},
   "source": [
    "We are a group of data scientists that representing a yoga studio. Our stakeholder, the marketing team want to develop a marketing campaign that target on Yoga Enthusiasts to sign up membership and classes with us. Instead of going for traditional way of marketing strategies, the team approach us and seek to maximize marketing resources with the effectiveness and efficiency marketing campaigns target on Yoga Enthusiasts. \n",
    "\n",
    "Our goal is to build an effective model that able to classify the *__yoga__* and *__non-yoga enthusiasts__* from the subreddit posts. This allow our marketing team to maximize the marketing spend to the correct target segment (yoga enthusiasts). We will also explore the differences between the subreddits (r/yoga and r/Meditation) in order to identify the Top Predictors and Trending Words for r/yoga. This is to better develop our marketing strategy, Yoga Classes and Packages.  \n",
    "\n",
    "We decided to scrap our dataset from Reddit posts - subreddits __r/yoga__ and __r/Meditation__ which both have similar in nature. Base on the subreddits post topics (title and selftext), we will then build an effective classification model to classify the posts accurately by evaluating the models using __accuracy__, __recall__ and __f1-score__. The classification models that we will build are `Logistic Regression`, `Multinomial Naive Bayes` and `Random Forest Classifier`.\n",
    "\n",
    "In order to evaluate success of our models, if our dataset is balanced in classes, then we will need to have high accuracy score, it means our model will have high predicted correctly outcomes out of overall predictions. However, if our dataset is imbalanced in classes, then we will need to have high f1-score which take a balance between precision and recall score. It is unrealistic to say that our model will be perfect or 100% accurate predict and generalize the datasets. Hence there will be errors (type I and type II). In this project, we can accept some type I error or False Positive, it means that predicted is yoga enthusiasts in fact is non-yoga enthusiasts. Although this is not our major target segment, there is a potential possibilities to convert this group (non-yoga enthusiasts) into dollar. On the other hand, we will need to minimize Type II error or False Negative as this means we predicted as non-yoga enthusiasts, in fact this is actual yoga enthusiasts. It means that we will miss out our target segments and we do not want our marketing spend miss out the targeted segments that highly to be converted into dollar. Hence, maximize recall score is needed in such situations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a06e4",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2555ff5",
   "metadata": {},
   "source": [
    "*__(Notebook: 1_Data_Gathering)__*\n",
    "- [1.0 Executive Summary](#1.0-Executive-Summary)\n",
    "\n",
    "- [2.0 Imports Data and Libraries](#2.0-Imports-Data-and-Libraries)\n",
    "    - [2.1 Scrapping Data: PushShift Reddit API](#2.1-Scrapping-Data:-PushShift-Reddit-API)\n",
    "    - [2.2 Export Raw Data](#2.2-Export-Raw-Data)\n",
    "\n",
    "*__(Notebook: 2_Data_Wrangling_EDA)__*\n",
    "\n",
    "- 3.0 Data Cleaning\n",
    "    - 3.1 Missing Values\n",
    "    - 3.2 Duplicates\n",
    "    - 3.3 Clean Irrelevant Rows\n",
    "    - 3.4 Clean Newline and Emoji\n",
    "    - 3.5 Clean HTML Encoded Characters\n",
    "    - 3.6 Clean URL Links\n",
    "    \n",
    "- 4.0 Data Preprocessing\n",
    "    - 4.1 Word Tokenizing Lemmatizing and Stemming\n",
    "    - 4.2 Word Count and Post Length\n",
    "    - 4.3 Data Dictionary\n",
    "\n",
    "- 5.0 Exploratory Data Analysis\n",
    "    - 5.1 Target Response: Subreddits\n",
    "    - 5.2 Number of Comments\n",
    "    - 5.3 Score\n",
    "    - 5.4 Upvote Ratio\n",
    "    - 5.5 Title Length and Word Count\n",
    "    - 5.6 Selftext Length and Word Count\n",
    "    - 5.7 Top Words Analysis\n",
    "    - 5.8 Combine and Export Cleaned Data\n",
    "    - 5.9 Additional: Sentiment Analysis\n",
    "\n",
    "*__(Notebook: 3_Modeling_Evaluation_Insights)__*\n",
    "\n",
    "- 6.0 Model Preparation\n",
    "    - 6.1 Baseline Model\n",
    "    - 6.2 Train Test Split\n",
    "    \n",
    "- 7.0 Modeling and Evaluation\n",
    "    - 7.1 Modeling Approach\n",
    "    - 7.2 Evaluation Approach\n",
    "    - 7.3 Logistic Regression\n",
    "    - 7.4 Multinomial Naive Bayes\n",
    "    - 7.5 Random Forest Classifier\n",
    "    - 7.6 Final Model\n",
    "    - 7.7 Model Insights\n",
    "\n",
    "- 8.0 Conclusions and Recommendations\n",
    "    - 8.1 Findings\n",
    "    - 8.2 Recommendations\n",
    "    - 8.3 Future Enhancements\n",
    "    - 8.4 Citations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96545fd9",
   "metadata": {},
   "source": [
    "## 1.0 Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4979f4f",
   "metadata": {},
   "source": [
    "Traditionally, our marketing approach on our targeted segments is (1) Need Based Approach and (2) Profilers Approach. Both approaches are good, however with the growth in the Internet and Social Media usages, people tend to access information easily and faster as compared to old days. People now getting familiar and adhere to Twitter, Facebook, Instagram, Reddit for social, posting, discussion as well as sharing thoughts. And these medias consists plenty of unstructured data eg. Images, Videos as well as text (reviews). According to [DAVID SAYCE](https://www.dsayce.com/social-media/tweets-day/) As of May 2020, Twitter every second, on average have around 6,000 tweets or, 350,000 tweets sent per minute or, 500 million tweets sent each day. Another example, according to [OBERLO](https://sg.oberlo.com/blog/reddit-statistics), Reddit have 52 million daily active users worldwide, users spend on average 10 min and 23 sec per visit on the site, there are currently more than 2.8 million subreddits and more than 130,000 active communities. With all these examples, it tell us that instead of adhere to traditional marketing approach, we can explore on more advance and scientific approach by analyzing all these data online and build a model which able to generalize for new data.\n",
    "\n",
    "We understand that by using traditional approach, it takes a lot of time to conduct surveys and research to get the right profiles of clients or understand the market trends, hiring sales person with high commissions to conduct need based approach sales and acquire the customers. All these are challenges we are facing and will eventually lead us losing of market competitiveness if no changes made.\n",
    "\n",
    "This project is to focus on building a text classifier to identify the posts from Reddit r/yoga and r/Meditation. From the effective model we build, we able to distinguish Yoga Enthusiasts and non-yoga Enthusiasts from the subreddit posts. It allow us to accurately classify and target on Yoga Enthusiasts. Eventually this will lead to maximize our marketing resources and convert to dollar for the company. Besides that, We also able to find out the top predictors or trending words for Yoga Enthusiasts. In other words, our effective model able to generalize for future new data and identify the top predictors and trending words in just one-click (few minutes) away. Marketing team can have significant reduction in manpower needed, money spend on surveys and researches and allocate the resources to right channel to improve company competitiveness in long run.\n",
    "\n",
    "In order to get the sufficient datasets, we scrapped 1500 \"submission\" posts from each subreddit using PushiShift Reddit API.\n",
    "After cleaning off the datasets, removed non-text and duplicates subreddit posts, we have total 1679 posts after combined both yoga and meditation datasets. Our text classifier focus on combination of title and selftext from subreddits. We also removed the stop words and lemmatized the text in order for better feature extraction via Count Vectorizer and TFIDF Vectorizer.\n",
    "\n",
    "As of the date we scrapped the datasets, we did exploratory data analysis and found that r/Meditation have more text post than r/yoga. It can infer that 36.8% of r/yoga posts are non-text (eg. videos, images and etc.) while 15.9% of r/Meditation posts are non-text. r/yoga have average higher number of comments (9.35) than r/Meditation (2.69) on the subreddit post. It shows that yoga community members are more actively in engaging by commenting in subreddits than meditation community. On the other hand, we found that r/Meditation posts have higher average score from upvote system (2.36) than r/yoga (1.49), it shows that r/Meditation community members prefer using the upvote system by showing their support instead of commenting. In term of length and word count on *Title and selftext*, r/Meditation have average title length of 50.17 and average word count of 8.52, selftext length of 657.43 and word count of 122.65 which these statistics are higher than r/yoga and shows that r/Meditation community members prefer expressing themself or sharing thought in text form instead of like r/yoga non-text form.\n",
    "\n",
    "Our baseline accuracy is 58.6% and it means that we can simply predict everything as the majority class and it will give us 58.6% accuracy. We then build 3 models from - *Logistic Regression, Multinomial Naive Bayes, Random Forest Classifier* for modeling. Eventually, Count Vectorizer with Multinomial Naive Bayes provides the less overfitting model and highest Test Accuracy (90.7%), Recall (85.2%) and F1-Score (88.3%) on classifying our target Yoga Enthusiasts correctly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c8ec14",
   "metadata": {},
   "source": [
    "## 2.0 Imports Data and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644f6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# for webscraping\n",
    "import requests, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384263e9",
   "metadata": {},
   "source": [
    "### 2.1 Scrapping Data: PushShift Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ab345",
   "metadata": {},
   "source": [
    "Our first task of the project is to collect the posts from subreddits r/yoga and r/Meditation. There are lot of different methods in web scrapping, we decided to use [PushShift Reddit API](https://github.com/pushshift/api) for our scrapping task. Pushshift API is fairly straightforward, however there is limitation in each request of scrapping to 100 posts. It means if we need 1500 posts from each subreddit, we need to scrap for 15 times and in between of scrapping we have put sleep time (2-3 seconds) in order to avoid tje server to be overloaded with too many requests in a very short period of time.\n",
    "\n",
    "To complete the project, we created a function for scrapping the subreddit posts 1500 each from r/yoga and r/Meditation. In order to avoid different datasets from each round of running the notebook, we decided to fix our `created_utc` at __1640908800__, in human-readable way is __Date and time (GMT): Friday, December 31, 2021 12:00:00 AM__. You can visit [epochconverter](https://www.epochconverter.com/) to convert Unix epoch time to human-readable date time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea8b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function scrap the posts from the subreddit\n",
    "\n",
    "def scrap_posts(subreddit, n_posts, created_utc):\n",
    "    posts = []\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    \n",
    "    bef_dict = {'before': created_utc}\n",
    "    \n",
    "    for i in range(n_posts):\n",
    "        params = {\n",
    "                'subreddit':subreddit,\n",
    "                'size': 100,\n",
    "                'before': bef_dict['before']\n",
    "                }\n",
    "            \n",
    "        res = requests.get(url, params)\n",
    "        \n",
    "        if res.status_code != 200:\n",
    "            print(f'Error Code {res.status_code}, {res.reason}')\n",
    "            break\n",
    "        \n",
    "        data = res.json()\n",
    "        posts.extend(data['data'])\n",
    "            \n",
    "        bef_dict['before'] = data['data'][-1]['created_utc']\n",
    "        \n",
    "        # generate random sleep time\n",
    "        sleep_time = np.random.randint(2,3)\n",
    "        time.sleep(sleep_time)\n",
    "        print(sleep_time)\n",
    "        print(f\"r/{subreddit} - Code:{res.status_code}, Status:{res.reason}\")\n",
    "    \n",
    "    # create dataframe for scrapped posts\n",
    "    df = pd.DataFrame(posts)\n",
    "    df['created'] = df['created_utc'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "    \n",
    "    # Stamping post and datetime while scraping \n",
    "    latest_post_stamped = datetime.fromtimestamp(df['created_utc'].iloc[0:].values[0])\n",
    "    last_post_stamped = datetime.fromtimestamp(df['created_utc'].iloc[-1:].values[0])\n",
    "    \n",
    "    print(f\"Scrapped {df.shape[0]} posts from {latest_post_stamped} to {last_post_stamped}\")\n",
    "    print()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba93afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "2\n",
      "r/yoga - Code:200, Status:OK\n",
      "Scrapped 1500 posts from 2021-12-31 07:55:04 to 2021-11-07 19:16:53\n",
      "\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "2\n",
      "r/Meditation - Code:200, Status:OK\n",
      "Scrapped 1500 posts from 2021-12-31 07:38:50 to 2021-12-03 11:39:22\n",
      "\n",
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Scrap for 1500 posts from r/yoga\n",
    "yoga = scrap_posts('yoga',15, 1640908800)\n",
    "\n",
    "# Scrap for 1500 posts from r/Meditation\n",
    "meditation = scrap_posts('Meditation',15, 1640908800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79129897",
   "metadata": {},
   "source": [
    "We have scrapped total 3000 posts from reddit. 1500 posts from r/yoga between *2021-12-31 07:55:04 to 2021-11-07 19:16:53* and 1500 posts from r/Meditation between *2021-12-31 07:38:50 to 2021-12-03 11:39:22*.\n",
    "\n",
    "With the PushShift API, it allow us to capture few important features:\n",
    "- total 3000 posts from subreddits inclusive:\n",
    "    - Name of subreddit\n",
    "    - Title of the subreddit post\n",
    "    - Selftext of the subreddit post\n",
    "    - Text post (True or False)\n",
    "    - Number of comments \n",
    "    - Created Date Time (Unix Epoch Time) \n",
    "    - Indicators of Subreddit post Removal\n",
    "    - Name of Author\n",
    "    - Permalink for the submission\n",
    "    - Score of subreddit post\n",
    "    - Upvote Ratio\n",
    "    \n",
    "We will explore these features and datasets on our next notebook - *__2_Data_Wrangling_EDA__*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273032a",
   "metadata": {},
   "source": [
    "### 2.2 Export Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7e9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets\n",
    "yoga.to_csv('../datasets/yoga_raw.csv', index=False)\n",
    "meditation.to_csv('../datasets/meditation_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d5b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
